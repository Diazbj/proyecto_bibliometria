@inproceedings{10.1145/3544549.3573794,
author = {Muller, Michael and Chilton, Lydia B and Kantosalo, Anna and Liao, Q. Vera and Maher, Mary Lou and Martin, Charles Patrick and Walsh, Greg},
title = {GenAICHI 2023: Generative AI and HCI at CHI 2023},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3573794},
doi = {10.1145/3544549.3573794},
abstract = {This workshop applies human centered themes to a new and powerful technology, generative artificial intelligence (AI). Unlike AI systems that produce decisions or descriptions, generative AI systems can produce new and creative content that can include images, texts, music, video, code, and other forms of design. The results are often similar to results produced by humans. However, it is not yet clear how humans make sense of generative AI algorithms or their outcomes. It is also not yet clear how humans can control and more generally, interact with, these powerful capabilities in ethical ways. Finally, it is not clear what kinds of collaboration patterns will emerge when creative humans and creative technologies work together. Following a successful workshop in 2022, we convene the interdisciplinary research domain of generative AI and HCI. Participation in this invitational workshop is open to seasoned scholars and early career researchers. We solicit descriptions of completed projects, works-in-progress, and provocations. Together we will develop theories and practices in this intriguing new domain.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {350},
numpages = {7},
keywords = {Bias, Design, Generative AI, Uncertainty.},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3701571.3701572,
author = {Rahman, Parinda and Adaji, Ifeoma},
title = {Ethics in Persuasive Technologies: A Systematic Literature Review},
year = {2024},
isbn = {9798400712838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701571.3701572},
doi = {10.1145/3701571.3701572},
abstract = {Persuasive technologies, which are intended to change users’ attitudes or behaviors and encourage specific actions, are widely applied across various domains. However, the fine line between persuasion and coercion raises significant ethical concerns, which current literature only superficially addresses. This paper aims to deepen the understanding of factors influencing the ethical perception of persuasive technologies through a systematic literature review of 17 journal articles. The selected studies were analyzed using content analysis to identify key ethical factors. The findings indicate that factors such as autonomy, consent, data privacy, transparency, and addictive design strategies significantly influence users’ ethical perceptions across multiple application domains. Generative artificial intelligence (AI) technologies or AI agents, particularly applications like argumentative chatbots and storytelling robots, exhibit the highest number of ethical considerations. The study also notes thematic overlaps among many ethical factors, with the context and use case impacting ethical perceptions. Based on these results, this paper offers design recommendations and suggestions for the design of ethical persuasive technology applications.},
booktitle = {Proceedings of the International Conference on Mobile and Ubiquitous Multimedia},
pages = {106–118},
numpages = {13},
keywords = {ethics, persuasion, mobile applications, technologies},
location = {
},
series = {MUM '24}
}

@inproceedings{10.1145/3491101.3503719,
author = {Muller, Michael and Chilton, Lydia B and Kantosalo, Anna and Martin, Charles Patrick and Walsh, Greg},
title = {GenAICHI: Generative AI and HCI},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503719},
doi = {10.1145/3491101.3503719},
abstract = {This workshop applies human centered themes to a new and powerful technology, generative artificial intelligence (AI). Unlike AI systems that produce decisions or descriptions, generative AI systems can produce new and creative content that can include images, texts, music, video, and other forms of design. The results are often similar to results produced by humans. However, it is not yet clear how humans make sense of generative AI algorithms or their outcomes. It is also not yet clear how humans can control and more generally, interact with, these powerful capabilities. Finally, it is not clear what kinds of collaboration patterns will emerge when creative humans and creative technologies work together. It is time to convene the interdisciplinary research domain of generative AI and HCI. Participation in this invitational workshop is open to seasoned scholars and early career researchers. We solicit descriptions of completed projects, works-in-progress, and provocations. Together we will develop theories and practices in this intriguing new domain.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {110},
numpages = {7},
keywords = {Bias, Design, Generative AI, Uncertainty.},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.5555/3615924.3615942,
author = {Alexopoulos, Michelle and Lyons, Kelly and Mahetaji, Kaushar and Barnes, Marcus Emmanuel and Gutwillinger, Rogan},
title = {Gender Inference: Can ChatGPT Outperform Common Commercial Tools?},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {An increasing number of studies use gender information to un-derstand phenomena such as gender bias, inequity in access and participation, or the impact of the Covid pandemic response. Un-fortunately, most datasets do not include self-reported gender in-formation, which makes it necessary for researchers to infer gen-der from other information, such as from names or names and country information. In this paper, we compare the performance of the new generative Artificial Intelligence (AI) tool ChatGPT with three traditional commercially available list-based and ma-chine learning-based gender inference tools—Namsor, Gender-API, and genderize.io—on a unique dataset. Specifically, we use a large Olympic athlete dataset and report how variations in the input (e.g., first name and first \&amp; last name, with and without country information) impact the accuracy of their predictions. We find that Namsor is the best traditional commercially available tool. However, ChatGPT performs at least as well as Namsor and often outper-forms it, especially for the female sample when country and/or last name information is available. We conclude ChatGPT may be a cost-effective tool for gender prediction.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {161–166},
numpages = {6},
keywords = {Name-Based Gender Inference, ChatGPT, Performance Evaluation, Data Science and AI},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3727166.3727191,
author = {Menezes, Veena Priscilla and Chowdhury, Mohammad Jabed Morshed and Mahmood, Abdun},
title = {An Agentic Framework for Compliant, Ethical and Trustworthy GenAI Applications in Healthcare},
year = {2025},
isbn = {9798400715075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727166.3727191},
doi = {10.1145/3727166.3727191},
abstract = {Recent progress in generative artificial intelligence (GenAI) has yielded significant advancements in healthcare, affecting radiology, medical imaging, drug development, patient diagnostics, and supply chain optimisation. These innovations promise more improved diagnoses and time-saving cost-effectiveness. However, GenAI’s rapid implementation poses significant challenges for meeting regulatory, ethical, and trustworthiness standards. These challenges include data privacy issues, reproducibility concerns, algorithmic bias in training data causing disparities in outcomes, and a lack of transparency and explainability. Unresolved, these issues could negatively affect the public’s confidence in and perception of GenAI systems. Addressing these challenges, international AI governance frameworks, including the EU AI Act and WHO guidelines, prioritize regulatory adherence, trustworthiness, and the explainability of healthcare AI systems. While such frameworks have expanded, a deficiency remains in translating policy into effective compliance mechanisms. We propose a Compliance Agentic Model (CAM) framework to help organizations comply with GenAI and machine learning (ML)-based solutions. The CAM framework establishes trustworthiness in GenAI applications used in healthcare, ensuring alignment with organizational values and ethical standards to enhance accountability and regulatory adherence.},
booktitle = {Proceedings of the 2025 Australasian Computer Science Week},
pages = {48–54},
numpages = {7},
keywords = {GenAI, Healthcare, Agentic AI, Compliance},
location = {
},
series = {ACSW '25}
}

@inproceedings{10.1145/3641555.3704765,
author = {Liu, Rongxin and Malan, David J. and Zhukovets, Yuliia and Lloyd, Doug},
title = {Teaching with AI (GPT)},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3704765},
doi = {10.1145/3641555.3704765},
abstract = {Teaching computer science at scale can be challenging. From our experience in CS50, Harvard University's introductory course, we've seen firsthand the impactful role that generative artificial intelligence can play in education. Recognizing its potential and stakes, we integrated OpenAI's GPT into our own teaching methodology. The goal was to emulate a 1:1 teacher-to-student ratio, incorporating "pedagogical guardrails" to maintain instructional integrity. The result was a personalized, AI-powered bot in the form of a friendly rubber duck aimed at delivering instructional responses and troubleshooting without giving outright solutions. In this tutorial, we share our journey and offer insights into responsibly harnessing AI in educational settings. Participants will gain hands-on experience working with GPT through OpenAI's latest APIs, understanding and crafting prompts, answering questions using embedding-based search, and finally, collaboratively building their own AI chatbot. Ultimately, we'll not only share lessons learned from our own approach but also equip educators hands-on with the knowledge and tools with which they, too, can implement these technologies in their unique teaching environments.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1773},
numpages = {1},
keywords = {AI, AI ethics, ChatGPT, GPT, generative AI, programming, prompt, prompt engineering},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inbook{10.5555/3716662.3716743,
author = {Luna, Jose and Tan, Ivan and Xie, Xiaofei and Jiang, Lingxiao},
title = {Navigating Governance Paradigms: A Cross-Regional Comparative Study of Generative AI Governance Processes \&amp; Principles},
year = {2025},
publisher = {AAAI Press},
abstract = {As Generative Artificial Intelligence (GenAI) technologies evolve at an unprecedented rate, global governance approaches struggle to keep pace with the technology, highlighting a critical issue in the governance adaptation of significant challenges. Depicting the nuances of nascent and diverse governance approaches based on risks, rules, outcomes, principles, or a mix, across different regions around the globe, is fundamental to discern discrepancies and convergences, and to shed light on specific limitations that need to be addressed, thereby facilitating the safe and trustworthy adoption of GenAI. In response to the need and the evolving nature of GenAI, this paper seeks to provide a collective view of different governance approaches around the world. Our research introduces a Harmonized GenAI Framework, "H-GenAIGF", based on the current governance approaches of six regions: (European Union (EU), United States (US), China (CN), Canada (CA), United Kingdom (UK), and Singapore (SG)). We have identified four constituents, fifteen processes, twenty-five sub-processes, and nine principles that aid the governance of GenAI, thus providing a comprehensive perspective on the current state of GenAI governance. In addition, we present a comparative analysis to facilitate identification of common ground and distinctions based on coverage of the processes by each region. The results show that risk-based approaches allow for better coverage of the processes, followed by mixed approaches. Other approaches lag behind, covering less than 50\% of the processes. Most prominently, the analysis demonstrates that amongst the regions, only one process aligns across all approaches, highlighting the lack of consistent and executable provisions. Moreover, our case study on ChatGPT reveals process coverage deficiency, showing that harmonization of approaches is necessary to find alignment for GenAI governance.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {917–931},
numpages = {15}
}

@inproceedings{10.1145/3699538.3699546,
author = {Keuning, Hieke and Alpizar-Chacon, Isaac and Lykourentzou, Ioanna and Beehler, Lauren and K\"{o}ppe, Christian and de Jong, Imke and Sosnovsky, Sergey},
title = {Students' Perceptions and Use of Generative AI Tools for Programming Across Different Computing Courses},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699546},
doi = {10.1145/3699538.3699546},
abstract = {Investigation of students’ perceptions and opinions on the use of generative artificial intelligence (GenAI) in education is a topic gaining much interest. Studies addressing this are typically conducted with large heterogeneous groups, at one moment in time. However, how students perceive and use GenAI tools can potentially depend on many factors, including their background knowledge, familiarity with the tools, and the learning goals and policies of the courses they are taking. In this study we explore how students following computing courses use GenAI for programming-related tasks across different programs and courses: Bachelor and Master, in courses in which learning programming is the learning goal, courses that require programming as a means to achieve another goal, and in courses in which programming is optional, but can be useful. We are also interested in changes over time, since GenAI capabilities are changing at a fast pace, and users are adopting GenAI increasingly. We conducted three consecutive surveys (fall ‘23, winter ‘23, and spring ‘24) among students of all computing programs of a large European research university. We asked questions on the use in education, ethics, and job prospects, and we included specific questions on the (dis)allowed use of GenAI tools in the courses they were taking at the time. We received 264 responses, which we quantitatively and qualitatively analyzed, to find out how students have employed GenAI tools across 59 different computing courses, and whether the opinion of an average student about these tools evolves over time. Our study contributes to the emerging discussion of how to differentiate GenAI use across different courses, and how to align its use with the learning goals of a computing course.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {14},
numpages = {12},
keywords = {Generative AI, Large Language Models, Computing Education, Programming Courses},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3643834.3661624,
author = {Uusitalo, Severi and Salovaara, Antti and Jokela, Tero and Salmimaa, Marja},
title = {”Clay to Play With”: Generative AI Tools in UX and Industrial Design Practice},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661624},
doi = {10.1145/3643834.3661624},
abstract = {Generative artificial intelligence (GAI) is transforming numerous professions, not least various fields intimately relying on creativity, such as design. To explore GAI’s adoption and appropriation in design, an interview-based study probed 10 specialists in user experience and industrial design, with varying tenure and GAI experience, for their adoption/application of GAI tools, reasons for not using them, problems with ownership and agency, speculations about the future of creative work, and GAI tools’ roles in design sensemaking. Insight from reflexive thematic analysis revealed wide variation in attitudes toward GAI tools – from threat-oriented negative appraisals to identification of empowerment opportunities – which depended on the sense of agency and perceived control. The paper examines this finding in light of the Coping Model of User Adaptation and discusses designers’ metacognitive skills as possible underpinnings for their attitudes. Avenues for further research are identified accordingly.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1566–1578},
numpages = {13},
keywords = {UX design, coping model of user adaptation, design, generative AI, industrial design, metacognition},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3604237.3626838,
author = {Yun, Jiseon and Sohn, Jae Eui and Kyeong, Sunghyon},
title = {Fine-Tuning Pretrained Language Models to Enhance Dialogue Summarization in Customer Service Centers},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626838},
doi = {10.1145/3604237.3626838},
abstract = {The application of pretrained language models in real-world business domains has gained significant attention. However, research on the practical use of generative artificial intelligence (AI) to address real-world downstream tasks is limited. This study aims to enhance the routine tasks of customer service (CS) representatives, particularly in the finance domain, by applying a fine-tuning method to dialogue summarization in CS centers. KakaoBank handles an average of 15,000 CS calls daily. By employing a fine-tuning method using real-world CS dialogue data, we can reduce the time required to summarize CS dialogues and standardize summarization skills. To ensure effective dialogue summarization in the finance domain, pretrained language models should acquire additional knowledge and skills, such as specific knowledge of financial products, problem-solving abilities, and the capacity to handle emotionally charged customers. In this study, we developed a reference fine-tuned model using Polyglot-Ko (5.8B) as the baseline PLM and a dataset containing a wide range of zero-shot instructions and partially containing summarization instructions. We compared this reference model with another model fine-tuned using KakaoBank’s CS dialogues and summarization data as the instruct dataset. The results demonstrated that the fine-tuned model based on KakaoBank’s internal datasets outperformed the reference model, showing a 199\% and 12\% improvement in ROUGE-L and RDASS, respectively. This study emphasizes the significance of task-specific fine-tuning using appropriate instruct datasets for effective performance in specific downstream tasks. Considering its practical use, we suggest that fine-tuning using real-world instruct datasets is a powerful and cost-effective technique for developing generative AI in the business domain.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {365–373},
numpages = {9},
keywords = {Korean language model, dialogue summarization, fine-tuning, instruct tuning},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@article{10.1145/3593230,
author = {Brie, Paul and Burny, Nicolas and Slu\"{y}ters, Arthur and Vanderdonckt, Jean},
title = {Evaluating a Large Language Model on Searching for GUI Layouts},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {EICS},
url = {https://doi.org/10.1145/3593230},
doi = {10.1145/3593230},
abstract = {The field of generative artificial intelligence has seen significant advancements in recent years with the advent of large language models, which have shown impressive results in software engineering tasks but not yet in engineering user interfaces. Thus, we raise a specific research question: would an LLM-based system be able to search for relevant GUI layouts? To address this question, we conducted a controlled study evaluating how Instigator, an LLM-based system for searching GUI layouts of web pages by generative pre-trained training, would return GUI layouts that are relevant to a given instruction and what would be the user experience of (N =34) practitioners interacting with Instigator. Our results identify a very high similarity and a moderate correlation between the rankings of the GUI layouts generated by Instigator and the rankings of the practitioners with respect to their relevance to a given design instruction. We highlight the results obtained through thirteen UEQ+ scales that characterize the user experience of the practitioner with Instigator, which we use to discuss perspectives for improving such future tools.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {178},
numpages = {37},
keywords = {generative pre-training, gui design, gui layout, large language model, web pages}
}

@article{10.1145/3649223,
author = {Silva, Jorge Sassaki Resende and Cardoso, Paula Christina Figueira and De Bettio, Raphael Winckler and Tavares, Daniela Cardoso and Silva, Carlos Alberto and Watanabe, Willian Massami and Freire, Andr\'{E} Pimenta},
title = {In-Page Navigation Aids for Screen-Reader Users with Automatic Topicalisation and Labelling},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3649223},
doi = {10.1145/3649223},
abstract = {Navigation aids such as headers and internal links provide vital support for screen-reader users on web documents to grasp a document’s structure. However, when such navigation aids are unavailable or not appropriately marked up, this situation can cause serious difficulties. This article presents the design and evaluation of a tool for automatically generating navigation aids with headers and internal links for screen readers with topicalisation and labelling algorithms. The proposed tool uses natural language processing techniques to divide a web document into topic segments and label each segment in two cycles based on its content. We conducted an initial user study in the first cycle with eight blind and partially-sighted screen reader users. The evaluation involved tasks with questions answered by participants with information from texts with and without automatically generated headers. The results in the first cycle provided preliminary indicators of performance improvement and cognitive load reduction. The second cycle involved co-designing an improved version with two blind experts in web accessibility, resulting in a browser extension which injects automatically generated headers and in-page navigation with internal links, along with improvements in the generation of labels using OpenAI’s ChatGPT. The browser extension was evaluated by seven blind participants using the same four texts used to evaluate the preliminary prototype developed in the first cycle. With the two development cycles, the study provided important insights into the design of navigation aids for screen-reader users using natural language processing techniques, including the potential use of generative artificial intelligence for assistive technologies and limitations that need to be explored in future research.},
journal = {ACM Trans. Access. Comput.},
month = jul,
articleno = {12},
numpages = {45},
keywords = {Accessibility, natural language processing, screen readers, topic segmentation and labelling, large language models, assistive technologies}
}

@inproceedings{10.5555/3721488.3721721,
author = {Rivadeneira, Franco and Carcausto, Daniela and Ore, Clara and Saga, Gabriela and Vilca, Macarena and Arroyo, Dante},
title = {AI-Driven Design of a Robotic Companion with Feline-Inspired Behaviors for Stress Relief},
year = {2025},
publisher = {IEEE Press},
abstract = {The rising levels of stress and anxiety among university students have become increasingly alarming, largely driven by academic pressures, social expectations, and the transition to adulthood. These challenges often result in a decline in mental well-being, necessitating innovative solutions to help ease the burden. This paper presents the design and development process of Purry, a social robot inspired by cat behaviors, created to provide stress relief for university students. Purry was designed to offer emotional comfort in a unique way, harnessing the calming effects of pet-like interactions. The design process focused on achieving both aesthetic appeal and functional efficiency, combining modern and traditional technologies such as generative artificial intelligence and low-cost materials, following the double diamond methodology. The robot mimics feline behaviors like kneading and purring, exploring both active and passive tactile interactions, which have been shown to reduce stress through sensory stimulation. Iterative development cycles, guided by user feedback, led to significant advancements in balancing form and function. The final result is a robotic experience that combines emotional support with innovative design, offering a product that not only addresses students' mental health needs but also fosters an engaging and comforting presence in their environment.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1568–1572},
numpages = {5},
keywords = {active touch, design process, robot-human interaction},
location = {Melbourne, Australia},
series = {HRI '25}
}

@article{10.1145/3713075,
author = {El Saddik, Abdulmotaleb and Ahmad, Jamil and Khan, Mustaqeem and Abouzahir, Saad and Gueaieb, Wail},
title = {Unleashing Creativity in the Metaverse: Generative AI and Multimodal Content},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {7},
issn = {1551-6857},
url = {https://doi.org/10.1145/3713075},
doi = {10.1145/3713075},
abstract = {The metaverse presents an emerging creative expression and collaboration frontier where generative artificial intelligence (GenAI) can play a pivotal role with its ability to generate multimodal content from simple prompts. These prompts allow the metaverse to interact with GenAI, where context information, instructions, input data, or even output indications constituting the prompt can come from within the metaverse. However, their integration poses challenges regarding interoperability, lack of standards, scalability, and maintaining a high-quality user experience. This article explores how GenAI can productively assist in enhancing creativity within the contexts of the metaverse and unlock new opportunities. We provide a technical, in-depth overview of the different generative models for image, video, audio, and 3D content within the metaverse environments. We also explore the bottlenecks, opportunities, and innovative applications of GenAI from the perspectives of end users, developers, service providers, and AI researchers. This survey commences by highlighting the potential of GenAI for enhancing the metaverse experience through dynamic content generation to populate massive virtual worlds. Subsequently, we shed light on the ongoing research practices and trends in multimodal content generation, enhancing realism and creativity and alleviating bottlenecks related to standardization, computational cost, privacy, and safety. Last, we share insights into promising research directions toward the integration of GenAI with the metaverse for creative enhancement, improved immersion, and innovative interactive applications.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {186},
numpages = {43},
keywords = {Generative AI, Metaverse, Diffusion Models, Generative Adversarial Networks, Multimodal, Content Generation}
}

@inproceedings{10.1145/3675585.3675587,
author = {Angeles, Chito Naorbe and Samson, Brylle Dimaano and Mama, Bai Rafsan Zahna Ibad and Luriaga, Ronnie Lucero and Delizo, John Pierre Demata and Ching, Michelle Renee Domingo},
title = {Students'perception of the use of AI Detector System by faculty members in determining the originality of submitted academic requirements},
year = {2024},
isbn = {9798400717659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675585.3675587},
doi = {10.1145/3675585.3675587},
abstract = {Recent studies revealed an overwhelming concern about the misuse of generative Artificial Intelligence (AI) tools by students in completing academic requirements. The detection of AI-generated content using the naked eye was perceived to be difficult, hence the need for more accurate, reliable, and effective detection methods. As a countermeasure, educators are turning to AI content detectors and plagiarism checkers to ascertain the originality of submitted school requirements, raising concerns from students regarding the accuracy and reliability of these tools and the ethical implications and negative consequences of misclassification of genuinely original works as machine-generated outputs. By employing a holistic case study approach, the authors attempted to determine the perceptions of selected university students on the use of AI detection systems by faculty members in checking the originality and novelty of their academic outputs. Through the lenses of various normative ethical theories, the authors also analyzed the ethical issues and concerns raised by selected students to better understand their sentiments and the factors they believe could influence the faculty members' intention to adopt this emerging technology. The results of the study revealed that students have mixed attitudes and perceptions toward the faculty's intention to use AI detectors. While students perceived it as a means to encourage independent learning and critical thinking, they also expressed valid concerns regarding fairness, accuracy, and reliability, the impact on teacher-student trust, and the responsible use of the technology, among others. The participants also acknowledged the influence of other faculty members and the students' increasing dependence on AI as possible enablers of technology adoption while technological limitations, the teachers’ lack of technological skills, and age as perceived barriers. From an ethical view, the findings of the study highlighted the importance of transparency, fairness, privacy, and the need for a policy to regulate AI use.},
booktitle = {Proceedings of the 2024 8th International Conference on E-Commerce, E-Business, and E-Government},
pages = {56–61},
numpages = {6},
keywords = {AI Detectors, Ethical Theories, Generative AI, TPB},
location = {Ajman, United Arab Emirates},
series = {ICEEG '24}
}

@inproceedings{10.1145/3706468.3706483,
author = {Hedlin, Elias and Estling, Ludwig and Wong, Jacqueline and Demmans Epp, Carrie and Viberg, Olga},
title = {Got It! Prompting Readability Using ChatGPT to Enhance Academic Texts for Diverse Learning Needs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706483},
doi = {10.1145/3706468.3706483},
abstract = {Reading skills are crucial for students' success in education and beyond. However, reading proficiency among K-12 students has been declining globally, including in Sweden, leaving many underprepared for post-secondary education. Additionally, an increasing number of students have reading disorders, such as dyslexia, which require support. Generative artificial intelligence (genAI) technologies, like ChatGPT, may offer new opportunities to improve reading practices by enhancing the readability of educational texts. This study investigates whether ChatGPT-4 can simplify academic texts and which prompting strategies are most effective. We tasked ChatGPT to re-write 136 academic texts using four prompting approaches: Standard, Meta, Roleplay, and Chain-of-Thought. All four approaches improved text readability, with Meta performing the best overall and the Standard prompt sometimes creating texts that were less readable than the original. This study found variability in the simplified texts, suggesting that different strategies should be used based on the specific needs of individual learners. Overall, the findings highlight the potential of genAI tools, like ChatGPT, to improve the accessibility of academic texts, offering valuable support for students with reading difficulties and promoting more equitable learning opportunities.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {115–125},
numpages = {11},
keywords = {Analytics, Equity, Large language models, Literacy, Prompt engineering, Readability},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3643834.3661515,
author = {Dangol, Aayushi and Newman, Michele and Wolfe, Robert and Lee, Jin Ha and Kientz, Julie A. and Yip, Jason and Pitt, Caroline},
title = {Mediating Culture: Cultivating Socio-cultural Understanding of AI in Children through Participatory Design},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661515},
doi = {10.1145/3643834.3661515},
abstract = {The surge in access to and awareness of Generative Artificial Intelligence (GenAI) such as ChatGPT has sparked discussion over the necessary technological literacies and competencies needed to effectively engage with these systems. In this context, we explore AI as a tool that mediates cultural understanding and remediates human values – that are often influenced by biases and inequities. Using participatory design for learning with a group of 13 children (ages 8-13), we engaged in five co-design sessions featuring different modalities for socio-cultural approaches to AI literacy. We found that children were more aware of the cultural mediation aspect of AI when the content of the interaction aligned with their cultural background and context. This underscored the significance of aligning the representation of culture in these GenAI systems with people’s socio-cultural ecosystems in modern technological literacies. We conclude with design principles for a more critical and holistic approach to AI literacy.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1805–1822},
numpages = {18},
keywords = {AI Literacy, Cultural mediation, Generative AI, Participatory design},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3613904.3642861,
author = {Mahdavi Goloujeh, Atefeh and Sullivan, Anne and Magerko, Brian},
title = {Is It AI or Is It Me? Understanding Users’ Prompt Journey with Text-to-Image Generative AI Tools},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642861},
doi = {10.1145/3613904.3642861},
abstract = {Generative Artificial Intelligence (AI) has witnessed unprecedented growth in text-to-image AI tools. Yet, much remains unknown about users’ prompt journey with such tools in the wild. In this paper, we posit that designing human-centered text-to-image AI tools requires a clear understanding of how individuals intuitively approach crafting prompts, and what challenges they may encounter. To address this, we conducted semi-structured interviews with 19 existing users of a text-to-image AI tool. Our findings (1) offer insights into users’ prompt journey including structures and processes for writing, evaluating, and refining prompts in text-to-image AI tools and (2) indicate that users must overcome barriers to aligning AI to their intents, and mastering prompt crafting knowledge. From the findings, we discuss the prompt journey as an individual yet a social experience and highlight opportunities for aligning text-to-image AI tools and users’ intents.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {183},
numpages = {13},
keywords = {Prompt engineering, generative AI, text-to-image generation, user journey},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613905.3650763,
author = {Gmeiner, Frederic and Conlin, Jamie Lynn and Tang, Eric Handa and Martelaro, Nikolas and Holstein, Kenneth},
title = {An Evidence-based Workflow for Studying and Designing Learning Supports for Human-AI Co-creation},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650763},
doi = {10.1145/3613905.3650763},
abstract = {Generative artificial intelligence (GenAI) systems introduce new possibilities for enhancing professionals’ workflows, enabling novel forms of human–AI co-creation. However, professionals often struggle to learn to work with GenAI systems effectively. While research has begun to explore the design of interfaces that support users in learning to co-create with GenAI, we lack systematic approaches to investigate the effectiveness of these supports. In this paper, we present a systematic approach for studying how to support learning to co-create with GenAI systems, informed by methods and concepts from the learning sciences. Through an experimental case study, we demonstrate how our approach can be used to study and compare the impacts of different types of learning supports in the context of text-to-image GenAI models. Reflecting on these results, we discuss directions for future work aimed at improving interfaces for human–AI co-creation.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {42},
numpages = {15},
keywords = {Case Study, Generative AI, Human–AI Co-creation, Human–AI Interaction, Learning, Study Method, Support Interfaces},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3702138.3702145,
author = {Blancaflor, Eric B. and Abaleta, Raphael M. and Achacoso, Luke Martin D.L. and Amper, Alden Christian C. and Ampiloquio, Pfrancis Isaiah R.},
title = {Emerging Threat: The Use of AI Voice Cloning Software and Services to Deceive Victims Through Phone Conversations and its Potential Effects on the Filipino Population},
year = {2025},
isbn = {9798400717543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702138.3702145},
doi = {10.1145/3702138.3702145},
abstract = {Generative Artificial Intelligence (AI) tools have become increasingly advanced and accessible via the Internet. These advancements and upgraded accessibility of access have resulted in the usage of generative AI in phishing, which has increased the incidences&nbsp;of these attacks. The researchers explore AI voice phishing, or vishing, and its possible implications on the Filipino community by analyzing and reviewing existing literature on AI voice cloning and its application in vishing schemes. The review covers the definition of vishing and AI voice cloning, the methods malicious actors use to clone voices, the Philippines' cybersecurity posture and its current laws on AI vishing, real-life examples of AI vishing, how to protect against it, and the future of AI vishing, as well as the future direction of the study. The researchers ended the study by demanding and advocating additional research on AI detection and recognition, as well as the establishment and stronger implementation of developing legislation in the Philippines and other nations that prohibit the use of generative AI for illegal purposes.},
booktitle = {Proceeding of the 2024 5th Asia Service Sciences and Software Engineering Conference},
pages = {137–146},
numpages = {10},
keywords = {AI Vishing, AI Voice Cloning, Artificial Intelligence, Cybersecurity, Generative AI, Philippines, Voice Phishing},
location = {
},
series = {ASSE '24}
}

@inproceedings{10.1145/3630106.3659031,
author = {Schmitt, Vera and Villa-Arenas, Luis-Felipe and Feldhus, Nils and Meyer, Joachim and Spang, Robert P. and M\"{o}ller, Sebastian},
title = {The Role of Explainability in Collaborative Human-AI Disinformation Detection},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659031},
doi = {10.1145/3630106.3659031},
abstract = {Manual verification has become very challenging based on the increasing volume of information shared online and the role of generative Artificial Intelligence (AI). Thus, AI systems are used to identify disinformation and deep fakes online. Previous research has shown that superior performance can be observed when combining AI and human expertise. Moreover, according to the EU AI Act, human oversight is inevitable when using AI systems in a domain where fundamental human rights, such as the right to free expression, might be affected. Thus, AI systems need to be transparent and offer sufficient explanations to be comprehensible. Much research has been done on integrating eXplainability (XAI) features to increase the transparency of AI systems; however, they lack human-centered evaluation. Additionally, the meaningfulness of explanations varies depending on users’ background knowledge and individual factors. Thus, this research implements a human-centered evaluation schema to evaluate different XAI features for the collaborative human-AI disinformation detection task. Hereby, objective and subjective evaluation dimensions, such as performance, perceived usefulness, understandability, and trust in the AI system, are used to evaluate different XAI features. A user study was conducted with an overall total of 433 participants, whereas 406 crowdworkers and 27 journalists participated as experts in detecting disinformation. The results show that free-text explanations contribute to improving non-expert performance but do not influence the performance of experts. The XAI features increase the perceived usefulness, understandability, and trust in the AI system, but they can also lead crowdworkers to blindly trust the AI system when its predictions are wrong.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2157–2174},
numpages = {18},
keywords = {Collaborative disinformation detection, XAI, expert and lay people evaluation, transparent AI systems},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/3680532.3689592,
author = {Singh, Gurprit and Jakob, Wenzel},
title = {MCMC: Bridging Rendering, Optimization and Generative AI},
year = {2024},
isbn = {9798400711350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3680532.3689592},
doi = {10.1145/3680532.3689592},
abstract = {Generative artificial intelligence (AI) has made unprecedented advances in vision language models over the past two years. These advances are largely due to diffusion-based generative models, which are very stable and simple to train. These diffusion models are tasked to learn the underlying unknown distribution of the training data samples. During the generative process, new samples (images) are generated from this unknown high-dimensional distribution. Markov Chain Monte Carlo (MCMC) methods are particularly effective in drawing samples from complex, high-dimensional distributions. This makes MCMC methods an integral component for both the training and sampling phases of these models, ensuring accurate sample generation.Gradient-based optimization is at the core of modern generative models. The update step during the optimization forms a Markov chain where the new update depends only on the current state. This allows exploration of the parameter space in a memoryless manner, thus combining the benefits of gradient-based optimization and MCMC sampling. MCMC methods have shown an equally important role in physically based rendering where complex light paths are otherwise quite challenging to sample from simple importance sampling techniques.A lot of research is dedicated towards bringing physical realism to samples (images) generated from diffusion-based generative models in a data-driven manner, however, a unified framework connecting these techniques is still missing. In this course, we take the first steps toward understanding each of these components and exploring how MCMC could potentially serve as a bridge, linking these closely related areas of research. Our tutorial aims to provide necessary theoretical and practical tools to guide students, researchers and practitioners towards the common goal of generative physically based rendering. All Jupyter notebooks with demonstrations associated to this tutorial can be found on our project webpage https://sinbag.github.io/mcmc/.},
booktitle = {SIGGRAPH Asia 2024 Courses},
articleno = {8},
numpages = {27},
location = {Tokyo, Japan},
series = {SA Courses '24}
}

@inproceedings{10.5555/3539845.3540111,
author = {Mirka, Maxime and France-Pillois, Maxime and Sassatelli, Gilles and Gamati\'{e}, Abdoulaye},
title = {A generative AI for heterogeneous network-on-chip design space pruning},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Often suffering from under-optimization, Networks-on-Chip (NoCs) heavily impact the efficiency of domain-specific Systems-on-Chip. To cope with this issue, heterogeneous NoCs are promising alternatives. Nevertheless, the design of optimized NoCs satisfying multiple performance objectives is extremely challenging and requires significant expertise. Prior works failed to combine many objectives or required an extended design space exploration time. In this paper, we propose an approach based on generative artificial intelligence to help pruning complex design spaces for heterogeneous NoCs, according to configurable performance objectives. This is made possible by the ability of Generative Adversarial Networks to learn and generate relevant design candidates for the target NoCs. The speed and flexibility of our solution enable a fast generation of optimized NoCs that fit users' expectations. Through some experiments, we show how to obtain competitive NoC designs reducing the power consumption with no communication performance or area penalty compared to a given conventional NoC design.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {1135–1138},
numpages = {4},
keywords = {CAD, DSSoC, generative adversarial network, heterogeneous, machine learning, network-on-chip},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3613904.3642574,
author = {Young, Jordyn and Jawara, Laala M and Nguyen, Diep N and Daly, Brian and Huh-Yoo, Jina and Razi, Afsaneh},
title = {The Role of AI in Peer Support for Young People: A Study of Preferences for Human- and AI-Generated Responses},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642574},
doi = {10.1145/3613904.3642574},
abstract = {Generative Artificial Intelligence (AI) is integrated into everyday technology, including news, education, and social media. AI has further pervaded private conversations as conversational partners, auto-completion, and response suggestions. As social media becomes young people’s main method of peer support exchange, we need to understand when and how AI can facilitate and assist in such exchanges in a beneficial, safe, and socially appropriate way. We asked 622 young people to complete an online survey and evaluate blinded human- and AI-generated responses to help-seeking messages. We found that participants preferred the AI-generated response to situations about relationships, self-expression, and physical health. However, when addressing a sensitive topic, like suicidal thoughts, young people preferred the human response. We also discuss the role of training in online peer support exchange and its implications for supporting young people’s well-being. Disclaimer: This paper includes sensitive topics, including suicide ideation. Reader discretion is advised.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {1006},
numpages = {18},
keywords = {AI-Mediated Communication (AI-MC), Artificial Intelligence (AI), Chatbot, Human-AI Interaction (HAII), LLM, Mental Health, Peer Support, Social Support, Youth},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3629606.3629675,
author = {He, Qingyang and Zheng, Weicheng and Bao, Hanxi and Chen, Ruiqi and Tong, Xin},
title = {Exploring Designers’ Perceptions and Practices of Collaborating with Generative AI as a Co-creative Agent in a Multi-stakeholder Design Process: Take the Domain of Avatar Design as an Example},
year = {2024},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629606.3629675},
doi = {10.1145/3629606.3629675},
abstract = {Nowadays, the traditional workflow of designers’ completing complicated design tasks has undergone a profound transformation due to the pervasive intervention of generative artificial intelligence (AI) tools, especially when multi-stakeholder participation is getting involved in the design process. Yet we know little about the designers’ perceptions and practices of collaborating with generative AI as a co-creative agent within the context of multi-stakeholder participation. To investigate these questions, we took the domain of avatar design as an example and conducted a qualitative interview study with 21 expert avatar designers who have got different levels of experience and expertise in utilizing generative AI tools in their design workflow. We found that designers not only would fall in a dilemma when deciding whether to consider AI as a co-creative agent according to different stakeholders’ interests, but they also face many challenges in effectively co-creating with the current systems, including challenges in consistently adjusting AI outputs and getting design inspiration within the iterative generation process, etc. Based on our findings, we concluded both the epistemological and creative patterns of collaborating with generative AI and highlighted several design opportunities from both technical and ethical perspectives to better support future designer-AI co-creation.},
booktitle = {Proceedings of the Eleventh International Symposium of Chinese CHI},
pages = {596–613},
numpages = {18},
keywords = {AI-assisted Design, Avatar Design, Co-creation Experience, Generative AI, Human-AI Collaboration, Stakeholder Identification},
location = {Denpasar, Bali, Indonesia},
series = {CHCHI '23}
}

@inproceedings{10.1145/3657054.3657092,
author = {Mancera Andrade, Jos\'{e} Alberto and Ter\'{a}n, Luis},
title = {From GenAI to Political Profiling Avatars: A Data-Driven Approach to Crafting Virtual Experts for Voting Advice Applications},
year = {2024},
isbn = {9798400709883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657054.3657092},
doi = {10.1145/3657054.3657092},
abstract = {Voting advice applications (VAAs) are pivotal web-based tools that guide citizens to align with political parties and candidates that match their preferences. Traditional methods for creating candidate profiles predominantly rely on questionnaire responses, a time-intensive and costly process. To address these challenges, we introduce a data-centric methodology utilizing generative artificial intelligence (GenAI), culminating in creating political avatars. These political avatars are engineered using cutting-edge large language models (LLMs), including GPT-4 and Bard. They are adept at processing and interpreting data primarily sourced from Twitter and leveraging bespoke, self-trained datasets. Integrating advanced AI technology with diverse data sources equips political avatars with unprecedented analytical and predictive capabilities, setting a new standard in political analysis. Unlike traditional methods, political avatars are adept at emulating the responses of real politicians or experts, showcasing a remarkable capacity to interact with VAA surveys. This novel approach presents the potential to either compete with or enhance the insights traditionally obtained from human experts. Another critical aspect of our study is comparing political avatars and previous research employing question-answering (QA) models based on advanced natural language processing (NLP) techniques for political profiling. This comparative analysis reveals that Political Avatars offer a significantly more robust solution for profile construction. While QA models provide structured responses based on specific queries, political avatars bring an element of dynamism and depth, capable of generating nuanced, context-aware responses. This shift from static, questionnaire-based profiling to dynamic, AI-driven avatars marks a substantial leap in political analysis. Generative AI in crafting Political Avatars introduces a transformative element to data analysis. This approach facilitates a layered and more sophisticated interpretation of political stances, moving beyond the limitations of traditional profiling methods. By employing political avatars, our methodology not only streamlines the profiling process but also enriches the quality of insights derived, paving the way for a more nuanced understanding of the political landscape.},
booktitle = {Proceedings of the 25th Annual International Conference on Digital Government Research},
pages = {305–311},
numpages = {7},
keywords = {Bard, GPT-4, Generative AI, Large Language Models, Natural Language Processing, Political Avatars, Question Answering, Social Media, Voting Advice Applications},
location = {Taipei, Taiwan},
series = {dg.o '24}
}

@article{10.1145/3686803,
author = {Li, Jialong and Zhang, Mingyue and Li, Nianyu and Weyns, Danny and Jin, Zhi and Tei, Kenji},
title = {Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3686803},
doi = {10.1145/3686803},
abstract = {Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this article aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI’s within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.†},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = sep,
articleno = {13},
numpages = {60},
keywords = {Self-Adaptive Systems, MAPE, Generative AI, Large Language Model, diffusion model, survey}
}

@article{10.1145/3654768.3654775,
author = {Zhang, Lotus},
title = {Designing Accessible Content Creation Support with Blind and Low Vision Creators},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {137},
issn = {1558-2337},
url = {https://doi.org/10.1145/3654768.3654775},
doi = {10.1145/3654768.3654775},
abstract = {Today, digital creative tools are still largely designed without disabled people in mind and thus require retrofitting to achieve basic accessibility. As more creative tools begin to incorporate generative artificial intelligence, I propose a research agenda that centers the design of human-AI co-creation experiences on disabled creators and leverages such technology for accessibility from the start. Specifically, I focus on researching ways that AI-assisted creative tools could be designed to lift the expression ceiling and reduce effort for blind and low vision creators. Starting with a formative mixed-method study that uncovers the blind and low vision community's needs for visual content creation and editing support, this dissertation explores and designs accessible content creation support for three highly desired visual creative tasks: (1) private visual content obfuscation, (2) social media video editing, and (3) aesthetic visual content authoring. Together, I believe this dissertation will contribute actionable insights to lower the barriers to expressive and efficient digital content creation for blind and low vision creators.},
journal = {SIGACCESS Access. Comput.},
month = mar,
articleno = {7},
numpages = {1}
}

@article{10.1145/3652154,
author = {Russo, Daniel},
title = {Navigating the Complexity of Generative AI Adoption in Software Engineering},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3652154},
doi = {10.1145/3652154},
abstract = {This article explores the adoption of Generative Artificial Intelligence (AI) tools within the domain of software engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology Acceptance Model, the Diffusion of Innovation Theory, and the Social Cognitive Theory as guiding theoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares–Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {135},
numpages = {50},
keywords = {Generative AI, large language models, technology adaption, empirical software engineering}
}

@inproceedings{10.1145/3638067.3638100,
author = {Freire, Andr\'{e} Pimenta and Cardoso, Paula Christina Figueira and Salgado, Andr\'{e} de Lima},
title = {May We Consult ChatGPT in Our Human-Computer Interaction Written Exam? An Experience Report After a Professor Answered Yes},
year = {2024},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638067.3638100},
doi = {10.1145/3638067.3638100},
abstract = {Using ChatGPT in education presents challenges for evaluating students. It requires distinguishing between original ideas and those generated by the model, assessing critical thinking skills, and gauging subject mastery accurately, which can impact fair assessment practices. The Human-Computer Interaction course described in this experience report has enabled consultation with textbooks, slides and other materials for over five years. This experience report describes reflections regarding using ChatGPT as a source of consultation in a written HCI exam in 2023. The paper describes experiences with analysis of the types of questions ChatGPT was able to solve immediately without mediation and the types of questions that could benefit from ChatGPT’s assistance without compromising the assessment of higher-level learning outcomes that professors want to analyse in teaching HCI. The paper uses Bloom’s taxonomy to analyse different questions and abilities to be evaluated and how they can be solved solely by using ChatGPT. The paper discusses questions that need mediation, previous lived experience in class and understanding of the knowledge acquired in class that cannot be answered directly by copying and pasting questions into ChatGPT. The discussions can raise reflections on the learning outcomes that can be assessed in HCI written exams and how professors should reflect upon their experiences and expectations for exams in the age of growing generative artificial intelligence resources.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
articleno = {6},
numpages = {11},
keywords = {ChatGPT, HCI education, evaluation, open-book exams},
location = {Macei\'{o}, Brazil},
series = {IHC '23}
}

@inproceedings{10.1145/3670865.3673482,
author = {Castro, Francisco and Gao, Jian and Martin, S\'{e}bastien},
title = {Human-AI Interactions and Societal Pitfalls},
year = {2024},
isbn = {9798400707049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670865.3673482},
doi = {10.1145/3670865.3673482},
abstract = {When working with generative artificial intelligence (AI), users may see productivity gains, but content generated with the help of AI may not match their preferences exactly. The boost in productivity may come at the expense of users' idiosyncrasies, such as personal style and tastes, preferences we would naturally express without AI. To let users express their preferences, many AI systems let users edit their prompt (e.g., Midjourney) or allow more natural interactions (e.g., ChatGPT), and users can always review and edit the AI-generated output themselves. However, aligning a user's intentions with an AI's output can take time and may not always be worth it if the AI's first or default output "does the job." In short, users face a trade-off between AI output fidelity and communication cost. The purpose of this work is to examine the impact of this human-AI interaction on the AI-generated content we produce as a society.We propose a Bayesian model to study the societal consequences of human-AI interactions. For a given task, rational users can exchange information with the AI to align its output with their heterogeneous preferences. The AI has a knowledge of the distribution of preferences in the population and uses a Bayesian update to create the optimal output with maximal expected fidelity given the information shared by the user. Users choose the amount of information they share to maximize their utility, balancing the cost of communication with the fidelity of the output.We show that the interplay between individual users and AI may lead to societal challenges. Outputs may become more homogenized. The AI-generated output distribution has a lower variance than the users' preference distribution. And this phenomenon is exacerbated when AI-generated content is used to train the next generation of AI: we show numerically that the users' rational decisions and the AI's training process can mutually reinforce each other, leading to a homogenization "death spiral." We also study the effects of AI bias, identifying who benefits or loses when using an AI model that does not accurately reflect the population preference distribution. At the population level, the censoring type of bias (e.g., biasing against the more unique preferences) negatively impacts the population utility as a whole, especially users with uncommon preferences who rely on AI interactivity the most. On the other hand, directional biases (e.g., a slightly left-leaning AI) will influence the users' chosen output, leading to a societal bias. Nonetheless, our research also demonstrates that creating models that facilitate human-AI interactions can limit these risks and preserve the population preference diversity.A full version of this paper can be found at https://arxiv.org/abs/2309.10448.},
booktitle = {Proceedings of the 25th ACM Conference on Economics and Computation},
pages = {205},
numpages = {1},
keywords = {generative AI, bayesian model, homogenization, bias},
location = {New Haven, CT, USA},
series = {EC '24}
}

@article{10.1145/3733106,
author = {Li, Yuxin and Nie, Jiangtian and Li, Shaobo and Jin, Kebing and Tang, Jianhang and Zhang, Yang and Niyato, Dusit},
title = {Intermediary Output Caching for Diffusion Model-Based Text-to-Image GenAI Services in Edge Computing Networks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1539-9087},
url = {https://doi.org/10.1145/3733106},
doi = {10.1145/3733106},
abstract = {The remarkable advancement of generative artificial intelligence (GenAI) has driven revolutionary applications for text-to-image generation, like Stable Diffusion and Imagen. Especially, the diffusion model can generate stunning images from natural language descriptions by using a reverse continuous denoising process. However, the computation burden of diffusion model-based GenAI services poses a significant hurdle for their practical implementation. In this work, we propose a novel edge computing-assisted GenAI framework to enable efficient GenAI service provision, where the intermediate output generated by diffusion models can be cached on edge servers and reused by various users to improve edge computing resource utilization. Assuming the existence of causally correlated auxiliary information, a long-term caching problem is formulated under intra-time-slot caching constraints by considering various maximally reusable steps of diffusion model-based GenAI tasks and the available caching capacity at edge servers. By leveraging the Lyapunov optimization framework, we transform the time-average caching problem into several deterministic problems for different time slots. We develop a deep reinforcement learning-based caching (DRC) algorithm to obtain caching decisions in each time slot, where a deep neural network (DNN)-based caching action generation module and a model-based evaluation module are designed. Finally, we conduct extensive simulation experiments by comparing the DRC algorithm with benchmark algorithms. The simulation results depict that the proposed DRC algorithm can reduce response time and improve cache hit rates significantly. The code is available at https://gitee.com/pipihinsky/DRC.},
note = {Just Accepted},
journal = {ACM Trans. Embed. Comput. Syst.},
month = apr,
keywords = {GenAI services, diffusion model, intermediary output caching, edge computing}
}

@inproceedings{10.1145/3641555.3705064,
author = {Erez, Yael and Ayali, Lilach and Hazzan, Orit},
title = {Evolution of Students' Attitudes Towards the Use of Generative AI Tools in a CS1 Course: Implications for Instructors},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705064},
doi = {10.1145/3641555.3705064},
abstract = {Recent advancements in large language model-based generative artificial intelligence (GenAI) tools have transformed computer science education, presenting both opportunities and challenges. A study investigating students' attitudes toward these tools was conducted during an Introduction to Computer Science course. The target of the study was to gauge students' evolving attitudes toward using GenAI tools in the course, before, during and after ChatGPT was gradually assimilated into homework assignments. The study refers to three phases: preliminary phase, assimilation phase, and calibration stage, which currently takes place. Findings show that, in the preliminary phase, students appreciated the efficiency of GenAI tools offered but were concerned about developing a dependency on these tools and about ''cheating''. Findings from the assimilation phase indicate that consistent, guided exposure to GenAI tools positively shifted students' views, alleviating initial concerns and promoting a positive attitude toward using GenAI tools in the course. The targets of the calibration phase are: a) to examine how to leverage independent learning by formulating clear guidelines that can build trust in the technology and help overcome concerns regarding reliability and credibility; b) to check how GenAI can help students in a Introduction to Computer Science course acquire skills such as critical thinking and code comprehension. The study offers insights for educators on the integration of GenAI tools into computer science courses to enhance learning while maintaining academic integrity.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1740},
numpages = {1},
keywords = {critical thinking, cs1, generative ai, introduction to computer science, mixed methods, program comprehension, skills, students' attitudes},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3715275.3732118,
author = {Djeffal, Christian},
title = {Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and AI Interaction Design},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732118},
doi = {10.1145/3715275.3732118},
abstract = {Responsible prompt engineering has emerged as a critical pracitce for ensuring that generative artificial intelligence (AI) systems are aligned with ethical, legal, and social principles. As generative AI applications become increasingly powerful and ubiquitous, the way we instruct and interact with them through prompts has profound implications for fairness, accountability, and transparency. It is, therefore, necessary to examine how strategic prompt engineering can embed ethical and legal considerations and societal values directly into AI interactions, moving beyond mere technical optimization for functionality. This article proposes “Reflexive Prompt Engineering”, a comprehensive framework for responsible prompt engineering that encompasses five interconnected components: prompt design, system selection, system configuration, performance evaluation, and prompt management. Drawing from empirical evidence, the paper demonstrates how each component can be leveraged to promote improved societal outcomes while mitigating potential risks. The analysis reveals that effective prompt engineering requires a delicate balance between technical precision and ethical consciousness, combining the systematic rigor and focus on functionality with the nuanced understanding of social impact. Through examination of emerging practices, this article illustrates how responsible prompt engineering serves as a crucial connection between AI development and deployment, enabling organizations to align AI outputs without modifying underlying model architectures. This approach links with broader “Responsibility by Design” principles, embedding ethical considerations directly into the implementation process rather than treating them as post-hoc additions. The article concludes by identifying key research directions and practical guidelines for advancing the field of responsible prompt engineering as an essential component of AI literacy.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1757–1768},
numpages = {12},
keywords = {AI Ethics, AI Governance, AI alignment, Accountability, Human-AI Interaction, Prompt Engineering, Responsible AI},
location = {
},
series = {FAccT '25}
}

@inproceedings{10.1145/3708359.3712137,
author = {Santana, Vagner Figueredo de and Berger, Sara and Machado, Tiago and de Macedo, Maysa Malfiza Garcia and Sanctos, Cassia Sampaio and Williams, Lemara and Wu, Zhaoqing},
title = {Can LLMs Recommend More Responsible Prompts?},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712137},
doi = {10.1145/3708359.3712137},
abstract = {Human-Computer Interaction practitioners have been proposing best practices in user interface design for decades. However, generative Artificial Intelligence (GenAI) brings additional design considerations and currently lacks sufficient user guidance regarding affordances, inputs, and outputs. In this context, we developed a recommender system to promote responsible AI (RAI) practices while people prompt GenAI systems, by recommending addition of sentences based on social values and removal of harmful sentences. We detail a lightweight recommender system designed to be used in prompting-time and compare its recommendations to the ones provided by three base large language models (LLMs) and two LLMs fine-tuned for the task, i.e., recommending inclusion of sentences based on social values and removal of harmful sentences from a given prompt. Results indicate that our approach has the best F1-score balance in terms of recommendations for additions and removal of sentences to promote responsible prompts, while a fine-tuned model obtained the best F1-score for additions, and our approach obtained the best F1-score for removals of harmful sentences. In addition, fine-tuned models improved the objectiveness of responses by reducing the verbosity of generated content in 93\% when compared to the content generated by base models. Presented findings contribute to RAI by showing the limits and bias of existing LLMs in terms of recommendations on how to create more responsible prompts and how open-source technologies can fill this gap in prompting-time.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {298–313},
numpages = {16},
keywords = {Prompt Engineering, Responsible Prompting, Responsible AI, Recommender Systems, Recommendation Systems},
location = {
},
series = {IUI '25}
}

@inproceedings{10.1145/3643834.3661501,
author = {Zhang, Kejun and Cai, Shixuan and Yang, Wenjing and Wu, Wenqi and Shen, Hanshu},
title = {Exploring Optimal Combinations: The Impact of Sequential Multimodal Inspirational Stimuli in Design Concepts on Creativity},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661501},
doi = {10.1145/3643834.3661501},
abstract = {The continuous advancements in Generative Artificial Intelligence (GenAI) enable designers to efficiently obtain vast amounts of inspirational materials during the design conception phase. Various modalities of stimuli can impact designers’ creativity. However, the relationship between the sequence of different modalities (Text, Images, and Virtual Models) and creativity remains to be explored. In this study, we investigate the effects of 6 sequential combinations of 3 modalities on designers’ creativity. Designers (N=36) used the creative stimulation platform InspireMe, accessing inspirational stimuli in various sequences to execute their design tasks. We analyzed the impact on creativity through assessments of work innovation and user interviews. The findings indicate that the sequence of material combinations significantly influences designers’ creativity, with varying impacts on the different sub-attributes of creativity. Drawing on experimental findings and qualitative research, this paper introduces a creativity demand-matching inspirational stimulation interaction strategy—the CDI strategy—that assists designers in developing more targeted design solutions. Furthermore, we propose the 3C Framework, a design strategy for creativity stimulation platforms aimed at designers in the intelligent era. This framework aims to guide the development of creative stimulation platforms for designers in the intelligent era.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {2788–2801},
numpages = {14},
keywords = {analogical reasoning, creativity, design behavior, design inspiration},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3702879.3702888,
author = {Zheng, Yubao and Xue, Shituan and Long, Anlin and Jia, Fengjuan and Guo, Huazhan and Ge, Jingwei},
title = {Research and Development Practice of Intelligent Reservoir Dynamic Analysis in Qinghai Oilfield},
year = {2024},
isbn = {9798400710148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702879.3702888},
doi = {10.1145/3702879.3702888},
abstract = {With the rapid development of generative artificial intelligence, data-driven intelligent computation and analysis methods have been massively applied in the field of petroleum exploration and development. Qinghai Oilfield has accumulated a large amount of reservoir dynamic analysis data in long-term exploration and development, but the comprehensive use of data and intelligent application is not deep enough, which seriously affects the efficiency of reservoir dynamic analysis. In order to improve the efficiency of reservoir dynamics analysis and decision-making, an intelligent reservoir dynamics analysis platform based on a cloud platform and artificial intelligence analysis methods was established in Qinghai Oilfield. The intelligent reservoir dynamic analysis platform integrates heterogeneous data from multiple sources, establishes scenario-based reservoir dynamic analysis topics, forms a four-level dynamic analysis structure of well-well group-layer-reservoir, and implements functions such as reservoir evaluation indicators calculation, reservoir dynamic early warning analysis, reservoir development parameters optimization, and reservoir development evaluation. Based on this platform, Qinghai Oilfield has achieved the intelligent practice of reservoir dynamic analysis with integrated geological engineering technology. Through the intelligent practice of reservoir dynamic analysis, Qinghai Oilfield has greatly improved the efficiency of reservoir development and production, and the digital and intelligent development has been promoted.},
booktitle = {Proceedings of the 2024 2nd International Conference on Internet of Things and Cloud Computing Technology},
pages = {47–53},
numpages = {7},
keywords = {artificial intelligence, data-driven, intelligent oilfield, reservoir dynamics analysis},
location = {
},
series = {IoTCCT '24}
}

@inproceedings{10.1145/3626253.3633433,
author = {Liu, Rongxin and Zenke, Carter and Lloyd, Doug and Malan, David J.},
title = {Teaching with AI (GPT)},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633433},
doi = {10.1145/3626253.3633433},
abstract = {Teaching computer science at scale can be challenging. From our experience in CS50, Harvard University's introductory course, we've seen firsthand the impactful role that generative artificial intelligence can play in education. Recognizing its potential and stakes, we integrated OpenAI's GPT into our own teaching methodology. The goal was to emulate a 1:1 teacher-to-student ratio, incorporating "pedagogical guardrails" to maintain instructional integrity. The result was a personalized, AI-powered bot in the form of a friendly rubber duck aimed at delivering instructional responses and troubleshooting without giving outright solutions. We plan to share our journey and offer insights into responsibly harnessing AI in educational settings. Participants will gain hands-on experience working with GPT through OpenAI's APIs, understanding and crafting prompts, answering questions using embedding-based search, and finally, building their own AI chatbot. Ultimately, we'll not only share lessons learned from our own approach but also equip educators hands-on with the knowledge and tools with which they, too, can implement these technologies in their unique teaching environments.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1902},
numpages = {1},
keywords = {ai, artificial intelligence, chatgpt, ethics, generative ai, gpt, programming, prompt, prompt engineering},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3563703.3596652,
author = {El-Zanfaly, Dina and Huang, Yiwei and Dong, Yanwen},
title = {Sand-in-the-loop: Investigating embodied co-creation for shared understandings of generative AI},
year = {2023},
isbn = {9781450398985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563703.3596652},
doi = {10.1145/3563703.3596652},
abstract = {Generative Artificial Intelligence (AI) applications in creative practices have grown tremendously over the past few years. However, these generative AI applications lack clarity on how they work and operate for their users. Revealing how generative AI tools work enables designers to understand these tools’ limitations and capabilities. We developed a tangible interface with sand as a medium for human-AI co-creation, Sand Playground. It extends the work in human-AI drawing practices beyond two-dimensional digital surfaces. Sand playground has three co-drawing modes, artistic mimicry, zen garden and doodling. We conducted a user study with ten designers. One of our findings is that the interface enabled users to predict the AI agent’s actions. Our research introduces novel insights into the role of tangible interactions and physical interfaces in generative AI literacy and explainability in design tools.},
booktitle = {Companion Publication of the 2023 ACM Designing Interactive Systems Conference},
pages = {256–260},
numpages = {5},
keywords = {Explainable AI, Graspable AI, human-AI co-creation},
location = {Pittsburgh, PA, USA},
series = {DIS '23 Companion}
}

@proceedings{10.1145/3698205,
title = {L@S '25: Proceedings of the Twelfth ACM Conference on Learning @ Scale},
year = {2025},
isbn = {9798400712913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to present the Proceedings of the Twelfth Annual ACM Conference on Learning at Scale, L@S 2025, held July 21-23, 2025 in Palermo, Italy.L@S investigates large-scale, technology-mediated learning environments that typically have many active learners and few experts on hand to guide their progress or respond to individual needs. The conference was created by the Association for Computing Machinery (ACM), inspired by the emergence of Massive Open Online Courses (MOOCs) and the accompanying shift in thinking about education. Over the years, the conference has evolved and is now one of the most relevant venues for discussion of the highest quality research on how learning and teaching can be transformed by technology.The aim of L@S is to address real-world educational challenges and simultaneously contribute to a fundamental understanding of human learning. Together, our goal is to investigate how data and technology can be leveraged to create more responsible and effective learning experiences for all learners across the globe. The theme of L@S 2025 is Learning Synergies @ Scale. As technology continues to reshape education, at the L@S 2025 conference we explore how generative artificial intelligence (AI) and its new possibilities can be leveraged to better support learning across the lifespan. L@S 2025 explores the impact of AI on learning with an emphasis on scalability and the broader impact of technological advances in learning environments. As a community we address the challenges of applying scalable, technology-driven solutions in both structured institutional environments and dynamic, self-directed learning communities, especially those that provide actionable insights for equitable and effective adaptive learning experiences. Creating synergies between research and practice, and contributing to breaking down barriers between the two, we present research co-informed by practice at scale.},
location = {Palermo, Italy}
}

@article{10.1145/3729237,
author = {Greco, Salvatore and La Quatra, Moreno and Cagliero, Luca and Cerquitelli, Tania},
title = {Towards AI-Assisted Inclusive Language Writing in Italian Formal Communications},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3729237},
doi = {10.1145/3729237},
abstract = {Formal communications such as public calls, announcements, or regulations are supposed to exhibit respect for diversity in terms of gender, race, age, and disability. However, human writers often lack adequate inclusive writing skills. For instance, they tend to overuse the masculine as a neutral form, mainly because they are self-trained on biased text examples. To overcome this issue, we propose to leverage Generative Artificial Intelligence to support inclusive language writing. Focusing on formal Italian communications, we have designed and developed an AI-assisted tool for non-inclusive text detection and reformulation. Thanks to the joint work with a team of linguistic experts, we first define a set of linguistic criteria necessary to model inclusive writing forms in Italian. Based on these criteria, we collect and annotate a dataset of Italian administrative documents enriched with fine-grained inclusive annotations. Finally, we train deep learning models on the collected data for non-inclusive language detection and inclusive language reformulation tasks. We perform quantitative and human-driven evaluations on the trained models. The best detection model correctly classifies 89\% of the sentences, whereas the best reformulation model produces 73\% fully correct reformulations. Both models have been integrated into a writing assistance tool acting as a text proofreader and self-learning tool for non-expert writers, namely Inclusively. Once a non-inclusive piece of text is detected, the proposed approach suggests inclusive reformulations. The tool also provides explanations of the models’ outputs to increase system transparency. Furthermore, it allows expert end-users to provide further annotations for system fine-tuning. The trained models and the writing assistance tool are publicly available for research purposes.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {79},
numpages = {24},
keywords = {inclusive language, natural language processing, text classification, text generation}
}

@proceedings{10.1145/3656650,
title = {AVI '24: Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {AVI 2024 is the 17th edition of the International Conference on Advanced Visual Interfaces, held in Arenzano, Genoa (IT), in cooperation with ACM, ACM SIGCHI, ACM SIGMM, and ACM SIGWEB.Every two years since 1992, AVI has gathered a vast international community of experts with a wide range of backgrounds. Throughout three decades, AVI has gained and holds a prestigious position among International HCI conferences, boasting a dedicated nucleus of returning participants, but also providing a venue for young researchers to show their achievements and establish contacts with senior community members.AVI 2024 presents a broad and sound scientific program covering traditional AVI topics on information and data visualization, interaction with multimodal user interfaces, augmented and virtual reality, while also addressing emerging topics including the application of generative artificial intelligence in HCI design and evaluation.The program features the presentation of 21 long research papers and 28 short papers selected through a rigorous double-blind reviewing process and organized into sessions on 13 main topics. Furthermore, it includes the presentation of 48 poster papers, 9 demo papers, and 11 doctoral consortium papers, selected through a single-blind reviewing process. Finally, the rich and vibrant program includes 3 keynote talks, 3 tutorials, and 10 workshops addressing some of the most exciting issues in HCI.Submissions to AVI 2024 came from 34 different countries distributed in descending order in Europe, Asia, North America, South America, and Africa.},
location = {Arenzano, Genoa, Italy}
}

@article{10.1145/3736765,
author = {Li, Xiang and Chen, Pin-Yu and Wei, Wenqi},
title = {Where are We in Audio Deepfake Detection? A Systematic Analysis over Generative and Detection Models},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3736765},
doi = {10.1145/3736765},
abstract = {Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using generative Artificial Intelligence (AI) technology have made it possible to generate high-quality and realistic human-like audio. This introduces significant challenges to distinguishing AI-synthesized speech from the authentic human voice and could raise potential issues of misuse for malicious purposes such as impersonation and fraud, spreading misinformation, deepfakes, and scams. However, existing detection techniques for AI-synthesized audio have not kept pace and often exhibit poor generalization across diverse datasets. In this article, we introduce SONAR, a synthetic AI-Audio Detection Framework and Benchmark, aiming at providing a comprehensive evaluation for distinguishing cutting-edge AI-synthesized auditory content. SONAR includes a novel evaluation dataset sourced from 9 diverse audio synthesis platforms, including leading TTS providers and state-of-the-art TTS models. It is the first framework to uniformly benchmark AI-audio detection across both traditional and foundation model-based deepfake detection systems. Through extensive experiments, (1) we reveal the generalization limitations of existing detection methods and demonstrate that foundation models exhibit stronger generalization capabilities, which can be attributed to their model size and the scale and quality of pretraining data. (2) Our evaluation of the generalization across languages suggests that speech foundation models demonstrate robust cross-lingual generalization capabilities, maintaining strong performance across diverse languages despite being fine-tuned solely on English speech data. This finding also suggests that the primary challenges in audio deepfake detection are more closely tied to the realism and quality of synthetic audio rather than language-specific characteristics. (3) We also explore the effectiveness and efficiency of few-shot fine-tuning in improving generalization, highlighting its potential for tailored applications, such as personalized detection systems for specific entities or individuals. Code and dataset are available at .},
journal = {ACM Trans. Internet Technol.},
month = aug,
articleno = {20},
numpages = {19},
keywords = {Audio, deepfake detection, benchmark framework}
}

@inbook{10.1145/3713043.3731602,
author = {Morales-Navarro, Luis},
title = {Investigating Youth’s Technical and Ethical Understanding of Generative Language Models When Engaging in Construction and Deconstruction Activities},
year = {2025},
isbn = {9798400714733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713043.3731602},
abstract = {The widespread adoption of generative artificial intelligence/machine learning (AI/ML) technologies has increased the need to support youth in developing AI/ML literacies. However, most work has centered on preparing young people to use these systems, with less attention to how they can participate in designing and evaluating them. This study investigates how engaging young people in the design and auditing of generative language models (GLMs) may foster the development of their understanding of how these systems work from both technical and ethical perspectives. The study takes an in-pieces approach to investigate novices’ conceptions of GLMs. Such an approach supports the analysis of how technical and ethical conceptions evolve and relate to each other. I am currently conducting a series of participatory design workshops with sixteen ninth graders (ages 14–15) in which they will (a) build GLMs from a data-driven perspective that glassboxes how data shapes model performance and (b) audit commercial GLMs by repeatedly and systematically querying them to draw inferences about their behaviors. I will analyze participants’ interactions to identify ethical and technical conceptions they may exhibit while designing and auditing GLMs. Then I will investigate the contexts in which these conceptions emerge and how participants’ personal interests and prior experiences may relate to their conceptions. I will also conduct clinical interviews and use microgenetic knowledge analysis and ordered network analysis to investigate how participants’ ethical and technical conceptions of GLMs relate to each other and change after the workshop. The study will contribute (a) evidence of how engaging youth in design and auditing activities may support the development of ethical and technical understanding of GLMs and (b) an inventory of novice design and auditing practices that may support youth’s technical and ethical understanding of GLMs.},
booktitle = {Proceedings of the 24th Interaction Design and Children},
pages = {1172–1175},
numpages = {4}
}

@inproceedings{10.1145/3539618.3593069,
author = {White, Ryen W.},
title = {Tasks, Copilots, and the Future of Search},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3593069},
doi = {10.1145/3539618.3593069},
abstract = {Tasks are central to information retrieval (IR) and drive interactions with search systems [2, 4, 10]. Understanding and modeling tasks helps these systems better support user needs [8, 9, 11]. This keynote focuses on search tasks, the emergence of generative artificial intelligence (AI), and the implications of recent work at their intersection for the future of search. Recent estimates suggest that half of Web search queries go unanswered, many of them connected to complex search tasks that are ill-defined or multi-step and span several queries[6]. AI copilots, e.g., ChatGPT and Bing Chat, are emerging to address complex search tasks and many other challenges. These copilots are built on large foundation models such as GPT-4 and are being extended with skills and plugins. Copilots broaden the surface of tasks achievable via search, moving toward creation not just finding (e.g., interview preparation, email composition), and can make searchers more efficient and more successful.Users currently engage with AI copilots via natural language queries and dialog and the copilots generate answers with source attribution [7]. However, in delegating responsibility for answer generation, searchers also lose some control over aspects of the search process, such as directly manipulating queries and examining lists of search results [1]. The efficiency gains from auto-generating a single, synthesized answer may also reduce opportunities for user learning and serendipity. A wholesale move to copilots for all search tasks is neither practical nor necessary: model inference is expensive, conversational interfaces are unfamiliar to many users in a search context, and traditional search already excels for many types of task. Instead, experiences that unite search and chat are becoming more common, enabling users to adjust the modality and other aspects (e.g., answer tone) based on the task.The rise of AI copilots creates many opportunities for IR, including aligning generated answers with user intent, tasks, and applications via human feedback [3]; understanding copilot usage, including functional fixedness [5]; using context and data to tailor responses to people and situations (e.g., grounding, personalization); new search experiences (e.g., unifying search and chat); reliability and safety (e.g., accuracy, bias); understanding impacts on user learning and agency; and evaluation (e.g., model-based feedback, searcher simulations [12] repeatability). Research in these and related areas will enable search systems to more effectively utilize new copilot technologies together with traditional search to help searchers better tackle a wider variety of tasks.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {5–6},
numpages = {2},
keywords = {artificial intelligence, complex tasks, copilots, search experience, search systems, task intelligence, task models, tasks, web search},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3530190.3534817,
author = {Ge, Xiou and Goodwin, Richard T. and Yu, Haizi and Romero, Pablo and Abdelrahman, Omar and Sudhalkar, Amruta and Kusuma, Julius and Cialdella, Ryan and Garg, Nishant and Varshney, Lav R.},
title = {Accelerated Design and Deployment of Low-Carbon Concrete for Data Centers},
year = {2022},
isbn = {9781450393478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530190.3534817},
doi = {10.1145/3530190.3534817},
abstract = {Concrete is the most widely used engineered material in the world with more than 10 billion tons produced annually. Unfortunately, with that scale comes a significant burden in terms of energy, water, and release of greenhouse gases and other pollutants; indeed 8\% of worldwide carbon emissions are attributed to the production of cement, a key ingredient in concrete. As such, there is interest in creating concrete formulas that minimize this environmental burden, while satisfying engineering performance requirements including compressive strength. Specifically for computing, concrete is a major ingredient in the construction of data centers. In this work, we use conditional variational autoencoders (CVAEs), a type of semi-supervised generative artificial intelligence (AI) model, to discover concrete formulas with desired properties. Our model is trained just using a small open dataset from the UCI Machine Learning Repository joined with environmental impact data from standard lifecycle analysis. Computational predictions demonstrate CVAEs can design concrete formulas with much lower carbon requirements than existing formulations while meeting design requirements. Next we report laboratory-based compressive strength experiments for five AI-generated formulations, which demonstrate that the formulations exceed design requirements. The resulting formulations were then used by Ozinga Ready Mix—a concrete supplier—to generate field-ready concrete formulations, based on local conditions and their expertise in concrete design. Finally, we report on how these formulations were used in the construction of buildings and structures in a Meta data center in DeKalb, IL, USA. Results from field experiments as part of this real-world deployment corroborate the efficacy of AI-generated low-carbon concrete mixes.},
booktitle = {Proceedings of the 5th ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
pages = {340–352},
numpages = {13},
keywords = {artificial intelligence, concrete, sustainable building materials, variational autoencoders},
location = {Seattle, WA, USA},
series = {COMPASS '22}
}

@article{10.5555/3648699.3649055,
author = {Pillutla, Krishna and Liu, Lang and Thickstun, John and Welleck, Sean and Swayamdipta, Swabha and Zellers, Rowan and Oh, Sewoong and Choi, Yejin and Harchaoui, Zaid},
title = {MAUVE scores for generative models: theory and practice},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Generative artificial intelligence has made significant strides, producing text indistinguishable from human prose and remarkably photorealistic images. Automatically measuring how close the generated data distribution is to the target distribution is central to diagnosing existing models and developing better ones. We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore three approaches to statistically estimate these scores: vector quantization, non-parametric estimation, and classifier-based estimation. We provide statistical bounds for the vector quantization approach.Empirically, we find that the proposed scores paired with a range of f-divergences and statistical estimation methods can quantify the gaps between the distributions of humanwritten text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We demonstrate in the vision domain that MAUVE can identify known properties of generated images on par with or better than existing metrics. In conclusion, we present practical recommendations for using MAUVE effectively with language and image modalities.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {356},
numpages = {92},
keywords = {generative models, evaluation, divergence frontiers, neural text generation, large language models, f-divergences, statistical estimation}
}

@inproceedings{10.1145/3708359.3712166,
author = {Narechania, Arpit and Endert, Alex and Sinha, Atanu R},
title = {Guidance Source Matters: How Guidance from AI, Expert, or a Group of Analysts Impacts Visual Data Preparation and Analysis},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712166},
doi = {10.1145/3708359.3712166},
abstract = {The progress in generative Artificial Intelligence (AI) has fueled AI-powered tools like co-pilots and assistants to provision better guidance, particularly during data analysis. However, research on guidance has not yet examined the perceived efficacy of the source from which guidance is offered and the impact of this source on the user’s perception and usage of guidance. We ask whether users perceive all guidance sources as equal, with particular interest in three sources: (i) “AI,” (ii) “human expert,” and (iii) “a group of human analysts.” As a benchmark, we consider a fourth source, (iv) “unattributed guidance,” where guidance is provided without attribution to any source, enabling isolation of and comparison with the effects of source-specific guidance. We design a five-condition between-subjects study, with one condition for each of the four guidance sources and an additional (v) “no-guidance” condition, which serves as a baseline to evaluate the influence of any kind of guidance. We situate our study in a custom data preparation and analysis tool wherein we task users to select relevant attributes from an unfamiliar dataset to inform a business report. Depending on the assigned condition, users can request guidance, which the system then provides in the form of attribute suggestions. To ensure internal validity, we control for the quality of guidance across source-conditions. Through several metrics of usage and perception, we statistically test five preregistered hypotheses and report on additional analysis. We find that the source of guidance matters to users, but not in a manner that matches received wisdom. For instance, users utilize guidance differently at various stages of analysis, including expressing varying levels of regret, despite receiving guidance of similar quality. Notably, users in the AI condition reported both higher post-task benefit and regret. These findings strongly indicate the need to further understand how different guidance sources impact user behavior for designing effective guidance systems.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {789–809},
numpages = {21},
keywords = {guidance, artificial intelligence, human expert, groupthink, data preparation, visual data analysis},
location = {
},
series = {IUI '25}
}

@article{10.1145/3731559,
author = {Pezz\`{e}, Mauro and Abrah\~{a}o, Silvia and Penzenstadler, Birgit and Poshyvanyk, Denys and Roychoudhury, Abhik and Yue, Tao},
title = {A 2030 Roadmap for Software Engineering},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3731559},
doi = {10.1145/3731559},
abstract = {The landscape of software engineering has dramatically changed in recent years. The impressive advances of artificial intelligence are just the latest and most disruptive innovation that has remarkably changed the software engineering research and practice. This special issue shares a roadmap to guide the software engineering community in this confused era. This roadmap is the outcome of a 2-day intensive discussion at the 2030 Software Engineering workshop. The roadmap spotlights and discusses seven main landmarks in the new software engineering landscape: artificial intelligence for software engineering, human aspects of software engineering, software security, verification and validation, sustainable software engineering, automatic programming, and quantum software engineering. This editorial summarizes the core aspects discussed in the 37 papers that comprise the seven sections of the special issue and guides the interested readers throughout the issue. This roadmap is a living body that we will refine with follow-up workshops that will update the roadmap for a series of forthcoming ACM TOSEM special issues.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {118},
numpages = {55},
keywords = {A roadmap for software engineering, AI and software engineering, Human factor in software engineering, Automatic Programming, Sustainable software engineering, Quantum software engineering, AI for verification and validation, security and software engineering, generative AI for software engineering, Large language models for software engineering}
}

@inproceedings{10.1145/3704289.3704300,
author = {Xiong, Xiao-Gang and Zeng, Meng-Ting},
title = {Research hotspots and path evolution of generative AI development--A Bibliometric Analysis Based on CiteSpace},
year = {2025},
isbn = {9798400716980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704289.3704300},
doi = {10.1145/3704289.3704300},
booktitle = {Proceedings of the 2024 7th International Conference on Big Data and Education},
pages = {22–28},
numpages = {7},
keywords = {Citespace, bibliometrics, generative AI},
location = {
},
series = {ICBDE '24}
}

